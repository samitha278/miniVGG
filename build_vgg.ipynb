{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI6/LAZ7itAB5tokTeRM+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/miniVGG/blob/main/build_vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1BohYIwpIVZ9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Convolution function"
      ],
      "metadata": {
        "id": "2xiuBEVgYqg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel):\n",
        "\n",
        "  x,y = image.shape\n",
        "\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  for i in range(y-b):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(x-a):\n",
        "\n",
        "      temp = (image[i:i+b,j:j+a] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "IEuWS_j6Id_A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel)\n"
      ],
      "metadata": {
        "id": "aprY8O8mK08d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rPfuwUIQBgB",
        "outputId": "18a43713-502d-4b28-861c-ab2a13e6e196"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([29, 29])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution with stride"
      ],
      "metadata": {
        "id": "vHxs061xYu3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel,stride=(1,1)):\n",
        "\n",
        "  x,y = image.shape\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  r,c = stride\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  for i in range(0,y-b,c):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(0,x-a,r):\n",
        "\n",
        "      temp = (image[i:i+b,j:j+a] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "xQbUppBMRAlH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel,(2,1))      # 2: aloge side rows , 1: along side columns\n"
      ],
      "metadata": {
        "id": "8gnkQhdwdkx-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHRzV0k5doNH",
        "outputId": "4f52a3e7-c446-44a1-d5e0-7bc59833cb13"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([29, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Padding"
      ],
      "metadata": {
        "id": "R4eAXkpueKTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding(image, padding):\n",
        "\n",
        "\n",
        "\n",
        "  r,c = padding\n",
        "\n",
        "  if r>0 :\n",
        "    rows = torch.zeros((r,image.shape[1]))\n",
        "\n",
        "    image = torch.cat((rows , image , rows),dim=0)\n",
        "\n",
        "  if c>0:\n",
        "\n",
        "    columns = torch.zeros((c,image.shape[0]))\n",
        "\n",
        "    image = torch.cat((columns , image.T , columns),dim=0)\n",
        "\n",
        "\n",
        "  return image.T\n",
        "\n"
      ],
      "metadata": {
        "id": "kqn-dKa9L0vj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_padding(torch.randn(2,2),(2,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEZZngmkWXtv",
        "outputId": "f58a76e3-2aeb-49ed-fd00-9cac11aad209"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000, -0.0341,  0.1381,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000, -0.8526,  0.5391,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel,stride=(1,1),padding = (0,0)):\n",
        "\n",
        "\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  r,c = stride\n",
        "\n",
        "  image_pd = image if padding==(0,0) else add_padding(image,padding)\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  x,y = image_pd.shape\n",
        "\n",
        "  for i in range(0,y-b,c):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(0,x-a,r):\n",
        "\n",
        "      temp = (image_pd[i:i+b,j:j+a] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "8lou7779dt22"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel, padding=(2,2))\n"
      ],
      "metadata": {
        "id": "-isnZot9ZnKI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HJ2ScO0Z0fG",
        "outputId": "f54f55e6-dae5-4819-fb2c-9998071d006f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([33, 33])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple adding pad func"
      ],
      "metadata": {
        "id": "JgaVXqF6bDfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding2(matrix,padding):\n",
        "\n",
        "  n,m = matrix.shape\n",
        "\n",
        "  r,c = padding\n",
        "\n",
        "  padded_matrix = torch.zeros((n+r*2,m+c*2))\n",
        "\n",
        "  padded_matrix[r:r+n,c:c+m] = matrix\n",
        "\n",
        "  return padded_matrix"
      ],
      "metadata": {
        "id": "jI4qh6KpZ2kJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_padding2(torch.randn(2,2),(2,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaRBYT3Vcs_7",
        "outputId": "ba1ce8ff-511e-4733-81a3-86a869f945a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000, -0.5153,  0.4146,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000, -1.0275,  0.0473,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Dialation"
      ],
      "metadata": {
        "id": "zsARmMMPdaZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel,stride=(1,1),padding = (0,0),dilation=(1,1)):\n",
        "\n",
        "\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  r,c = stride\n",
        "\n",
        "  d1,d2 = dilation\n",
        "\n",
        "  image_pd = image if padding==(0,0) else add_padding2(image,padding)\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  x,y = image_pd.shape\n",
        "\n",
        "\n",
        "  for i in range(0,x-a*d1+1,r):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(0,y-b*d2+1,c):\n",
        "\n",
        "      temp = (image_pd[i:i+a*d1:d1,j:j+b*d2:d2] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "ag8GkDJzc-Vw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel,(2,2),(2,2) ,dilation=(2,2))"
      ],
      "metadata": {
        "id": "_HIaqGa0yArI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "id": "3bIGNsuTyIbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be1e371-1b1e-4b4a-d719-6f1d66fd74eb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test : comparision with pytorch conv2d"
      ],
      "metadata": {
        "id": "2PSgSdooBcum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "out_custom = Convolution(image, kernel, stride=(2,2), padding=(1,1), dilation=(2,2))\n",
        "\n",
        "\n",
        "img4d = image.unsqueeze(0).unsqueeze(0)       # (1,1,4,4)\n",
        "kernel4d = kernel.unsqueeze(0).unsqueeze(0) # (1,1,2,2)\n",
        "\n",
        "out_torch = F.conv2d(img4d, kernel4d, stride=(2,2), padding=(1,1), dilation=(2,2))\n",
        "\n",
        "print(\"Custom:\\n\", out_custom.shape)\n",
        "print(\"Torch:\\n\", out_torch.squeeze().shape)\n",
        "print(\"Match? ->\", torch.allclose(out_custom,out_torch.squeeze()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rxzc5-Ws5VR",
        "outputId": "3a07f0ea-66ab-4325-fc08-bda8e955544d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom:\n",
            " torch.Size([15, 15])\n",
            "Torch:\n",
            " torch.Size([15, 15])\n",
            "Match? -> True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All at once"
      ],
      "metadata": {
        "id": "iBcW_6DpIrsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding2(matrix,padding):\n",
        "  n,m = matrix.shape\n",
        "  r,c = padding\n",
        "\n",
        "  padded_matrix = torch.zeros((n+r*2,m+c*2))\n",
        "  padded_matrix[r:r+n,c:c+m] = matrix\n",
        "\n",
        "  return padded_matrix"
      ],
      "metadata": {
        "id": "HXDBUD8lI3lh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image, kernel, stride=(1,1), padding=(0,0), dilation=(1,1)):\n",
        "    a, b = kernel.shape\n",
        "    r, c = stride\n",
        "    d1, d2 = dilation\n",
        "\n",
        "    image_pd = image if padding == (0,0) else add_padding2(image, padding)\n",
        "    feature_map = []\n",
        "    x, y = image_pd.shape\n",
        "\n",
        "    for i in range(0, x - a*d1 + 1, r):\n",
        "        row = []\n",
        "        for j in range(0, y - b*d2 + 1, c):\n",
        "\n",
        "            temp = (image_pd[i:i+a*d1:d1, j:j+b*d2:d2] * kernel).sum(dim=(0,1))\n",
        "            row.append(temp.item())\n",
        "        feature_map.append(row)\n",
        "\n",
        "    return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "K1ajUxKNBmxD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.manual_seed(220064)\n",
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "out_custom = Convolution(image, kernel, stride=(2,2), padding=(1,1), dilation=(2,2))\n",
        "\n",
        "\n",
        "img4d = image.unsqueeze(0).unsqueeze(0)       # (1,1,4,4)\n",
        "kernel4d = kernel.unsqueeze(0).unsqueeze(0) # (1,1,2,2)\n",
        "\n",
        "out_torch = F.conv2d(img4d, kernel4d, stride=(2,2), padding=(1,1), dilation=(2,2))\n",
        "\n",
        "print(\"Custom:\\n\", out_custom.shape)\n",
        "print(\"Torch:\\n\", out_torch.squeeze().shape)\n",
        "print(\"Match? ->\", torch.allclose(out_custom,out_torch.squeeze()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwys6oPrI6YL",
        "outputId": "91c8903b-6eca-451f-84d7-9011601047fb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom:\n",
            " torch.Size([15, 15])\n",
            "Torch:\n",
            " torch.Size([15, 15])\n",
            "Match? -> True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test nn.Conv2d"
      ],
      "metadata": {
        "id": "sMbL7SCfLKbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(200)\n",
        "\n",
        "c_l = nn.Conv2d(2,4,3)      # in channel = 2, out channel = 4, kernel size = 3x3\n",
        "\n",
        "\n",
        "sd = c_l.__dict__['_parameters']\n",
        "wei = sd['weight']\n",
        "bias = sd['bias']\n",
        "\n",
        "img = torch.randn(2,12,12)\n"
      ],
      "metadata": {
        "id": "7JuIATtbLBP5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for outout channel 1\n",
        "out = Convolution(img[0],wei[0,0]) + Convolution(img[1],wei[0,1]) + bias[0]"
      ],
      "metadata": {
        "id": "HVFT9JngLuVx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_real = c_l(img)[0]"
      ],
      "metadata": {
        "id": "OAQ5iR73LUR1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(out_real,out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQDfyyqGNggE",
        "outputId": "7f029e62-f99d-4147-fa4a-6c6065917c82"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX5ZN2NwQm6x",
        "outputId": "a548f59f-d704-4619-913b-a2b90f42a380"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.1716, -0.8489,  0.0087,  0.3687,  0.0612, -0.4664, -0.0916,  0.3448,\n",
              "         -0.3278,  0.4495],\n",
              "        [-0.9561,  0.1782,  0.3831, -0.2250, -0.4388, -0.5568, -0.3419,  0.2183,\n",
              "          0.0076, -0.6042],\n",
              "        [ 0.0471, -0.4671, -0.2238, -0.1945,  0.2181, -0.4026, -0.5394, -0.6788,\n",
              "         -0.5775, -0.8300],\n",
              "        [-0.1201,  0.0371, -0.3132, -0.2208, -0.2247, -0.3121,  0.1181, -0.3183,\n",
              "          0.2815, -1.5969],\n",
              "        [ 0.3042,  0.3495, -0.1749,  0.1566, -1.6928, -0.1662,  0.3522, -0.1735,\n",
              "         -0.7710, -0.8561],\n",
              "        [ 0.1167,  0.8531,  0.7492,  0.0081, -0.7360, -0.0125, -0.1202, -1.4806,\n",
              "          0.8252,  0.3005],\n",
              "        [-0.3631, -0.4048,  1.2756,  0.3087, -1.7028,  0.7630,  0.2855, -1.1258,\n",
              "          0.5164,  0.3389],\n",
              "        [-0.2170,  0.4665, -0.0310,  0.3007,  0.1645, -0.0165, -0.6288, -0.9614,\n",
              "         -0.2822,  0.3014],\n",
              "        [-1.0514, -0.2228, -0.3419,  0.8720, -0.4499,  0.1352, -0.1190, -0.4014,\n",
              "          0.8269,  0.5460],\n",
              "        [ 0.5875,  0.6329, -0.9254, -1.2093, -0.4766, -0.2230,  0.2021, -0.4626,\n",
              "         -0.4579, -0.0720]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0,0].shape, bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4fcAhV2bFcm",
        "outputId": "1fc22b78-4835-4c9b-8d8e-3f63dde44652"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 3]),\n",
              " Parameter containing:\n",
              " tensor([-0.2148,  0.1413, -0.0320,  0.0114], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Convolution(img[0],wei[0,0]) + Convolution(img[1],wei[0,1]) #+ torch.tensor([-0.2148])  # bias braodcasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ISjRHFBbj2U",
        "outputId": "c36d10c4-0817-4590-818f-5c8e3b4c6c32"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9568, -0.6341,  0.2235,  0.5836,  0.2760, -0.2516,  0.1232,  0.5596,\n",
              "         -0.1130,  0.6643],\n",
              "        [-0.7413,  0.3930,  0.5980, -0.0102, -0.2240, -0.3420, -0.1271,  0.4332,\n",
              "          0.2224, -0.3893],\n",
              "        [ 0.2619, -0.2522, -0.0090,  0.0204,  0.4329, -0.1878, -0.3246, -0.4639,\n",
              "         -0.3626, -0.6152],\n",
              "        [ 0.0947,  0.2519, -0.0984, -0.0059, -0.0099, -0.0973,  0.3329, -0.1035,\n",
              "          0.4963, -1.3820],\n",
              "        [ 0.5190,  0.5643,  0.0400,  0.3714, -1.4779,  0.0486,  0.5670,  0.0413,\n",
              "         -0.5561, -0.6412],\n",
              "        [ 0.3316,  1.0679,  0.9641,  0.2230, -0.5212,  0.2023,  0.0947, -1.2658,\n",
              "          1.0401,  0.5153],\n",
              "        [-0.1482, -0.1899,  1.4905,  0.5235, -1.4880,  0.9779,  0.5003, -0.9110,\n",
              "          0.7312,  0.5538],\n",
              "        [-0.0021,  0.6814,  0.1839,  0.5156,  0.3793,  0.1983, -0.4140, -0.7465,\n",
              "         -0.0674,  0.5162],\n",
              "        [-0.8365, -0.0079, -0.1271,  1.0868, -0.2350,  0.3500,  0.0959, -0.1865,\n",
              "          1.0418,  0.7609],\n",
              "        [ 0.8024,  0.8478, -0.7106, -0.9944, -0.2618, -0.0082,  0.4169, -0.2478,\n",
              "         -0.2430,  0.1428]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv2d Class"
      ],
      "metadata": {
        "id": "slGCyBanLXwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               in_channels, out_channels, kernel_size,\n",
        "               stride = 1, padding = 0, dilation= 1,\n",
        "               bias = True\n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "    assert kernel_size%2==1 , \"kernel size must be odd\"\n",
        "\n",
        "    self.weights = torch.randn((out_channels,in_channels,kernel_size,kernel_size)) * (in_channels**-0.5)\n",
        "    self.bias = torch.randn(out_channels) if bias else None\n",
        "\n",
        "\n",
        "\n",
        "    #variables\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    self.dilation = dilation\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    B,C,H,W = x.shape\n",
        "\n",
        "    H_out= W_out = ((H+2*self.padding-(self.dilation*self.kernel_size-1)-1)//self.stride)+1    # calculate output size\n",
        "\n",
        "    x = x if self.padding == 0 else x.add_padding(self.padding)     #adding padding\n",
        "\n",
        "    self.feature_map = torch.zeros((B,self.out_channels,H_out,W_out))    #output feature map\n",
        "\n",
        "    for b in range(B):\n",
        "      for out_c in range(self.out_channels):\n",
        "\n",
        "\n",
        "        #Convolution\n",
        "        for h in range(H_out):\n",
        "          for w in range(W_out):\n",
        "\n",
        "            #feature_map[b,out_c,h,w] = (x[i:i+a*d1:d1, j:j+b*d2:d2] * self.weight[out_c]).sum(dim=(0,1))\n",
        "\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return self.feature_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def add_padding(self,padding):\n",
        "    B,C,H,W = self.shape\n",
        "    p = padding\n",
        "\n",
        "    padded_x = torch.zeros((B,C,H+p*2,W+p*2))\n",
        "    padded_x[:,:,p:p+H,p:p+W] = self\n",
        "\n",
        "    return padded_x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iEg1eSdPJFIP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Conv2d()\n",
        "\n",
        "\"\"\"\n",
        "in_channels: int, out_channels: int, kernel_size: _size_2_t,\n",
        "stride: _size_2_t = 1, padding: _size_2_t | str = 0, dilation: _size_2_t = 1,\n",
        "groups: int = 1, bias: bool = True, padding_mode: str = \"zeros\",\n",
        "device: Any | None = None, dtype: Any | None = None) -> None\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "z7nsaCzUQoLR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7a08fd9e-3257-4e1b-814d-eea0003583fe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nin_channels: int, out_channels: int, kernel_size: _size_2_t, \\nstride: _size_2_t = 1, padding: _size_2_t | str = 0, dilation: _size_2_t = 1, \\ngroups: int = 1, bias: bool = True, padding_mode: str = \"zeros\", \\ndevice: Any | None = None, dtype: Any | None = None) -> None\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgiMTyF2YZP1"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}