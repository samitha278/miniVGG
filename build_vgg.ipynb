{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/vP/JkbB3FIAUI/PChESi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/miniVGG/blob/main/build_vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1BohYIwpIVZ9"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Convolution function"
      ],
      "metadata": {
        "id": "2xiuBEVgYqg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel):\n",
        "\n",
        "  x,y = image.shape\n",
        "\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  for i in range(y-b):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(x-a):\n",
        "\n",
        "      temp = (image[i:i+b,j:j+a] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "IEuWS_j6Id_A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel)\n"
      ],
      "metadata": {
        "id": "aprY8O8mK08d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rPfuwUIQBgB",
        "outputId": "3403a6bd-f09d-480f-b339-2b784a652944"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([29, 29])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution with stride"
      ],
      "metadata": {
        "id": "vHxs061xYu3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel,stride=(1,1)):\n",
        "\n",
        "  x,y = image.shape\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  r,c = stride\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  for i in range(0,y-b,c):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(0,x-a,r):\n",
        "\n",
        "      temp = (image[i:i+b,j:j+a] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "xQbUppBMRAlH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel,(2,1))      # 2: aloge side rows , 1: along side columns\n"
      ],
      "metadata": {
        "id": "8gnkQhdwdkx-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHRzV0k5doNH",
        "outputId": "54e0e589-606b-47bd-ff74-7cac3840fa91"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([29, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Padding"
      ],
      "metadata": {
        "id": "R4eAXkpueKTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding(image, padding):\n",
        "\n",
        "\n",
        "\n",
        "  r,c = padding\n",
        "\n",
        "  if r>0 :\n",
        "    rows = torch.zeros((r,image.shape[1]))\n",
        "\n",
        "    image = torch.cat((rows , image , rows),dim=0)\n",
        "\n",
        "  if c>0:\n",
        "\n",
        "    columns = torch.zeros((c,image.shape[0]))\n",
        "\n",
        "    image = torch.cat((columns , image.T , columns),dim=0)\n",
        "\n",
        "\n",
        "  return image.T\n",
        "\n"
      ],
      "metadata": {
        "id": "kqn-dKa9L0vj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_padding(torch.randn(2,2),(2,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEZZngmkWXtv",
        "outputId": "a88e5ca9-c651-4568-9732-bb22054cafb5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000, -0.2001,  1.4345,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000, -0.1159,  0.1599,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel,stride=(1,1),padding = (0,0)):\n",
        "\n",
        "\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  r,c = stride\n",
        "\n",
        "  image_pd = image if padding==(0,0) else add_padding(image,padding)\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  x,y = image_pd.shape\n",
        "\n",
        "  for i in range(0,y-b,c):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(0,x-a,r):\n",
        "\n",
        "      temp = (image_pd[i:i+b,j:j+a] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "8lou7779dt22"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel, padding=(2,2))\n"
      ],
      "metadata": {
        "id": "-isnZot9ZnKI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HJ2ScO0Z0fG",
        "outputId": "5d1d35ab-aacc-48d0-bdeb-ef876fa2722c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([33, 33])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple adding pad func"
      ],
      "metadata": {
        "id": "JgaVXqF6bDfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding2(matrix,padding):\n",
        "\n",
        "  n,m = matrix.shape\n",
        "\n",
        "  r,c = padding\n",
        "\n",
        "  padded_matrix = torch.zeros((n+r*2,m+c*2))\n",
        "\n",
        "  padded_matrix[r:r+n,c:c+m] = matrix\n",
        "\n",
        "  return padded_matrix"
      ],
      "metadata": {
        "id": "jI4qh6KpZ2kJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_padding2(torch.randn(2,2),(2,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaRBYT3Vcs_7",
        "outputId": "8e3773df-2806-4dd4-9cc7-e4ebf7b33984"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.1276, -0.6512,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000, -0.0422, -0.8412,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Dialation"
      ],
      "metadata": {
        "id": "zsARmMMPdaZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel,stride=(1,1),padding = (0,0),dilation=(1,1)):\n",
        "\n",
        "\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  r,c = stride\n",
        "\n",
        "  d1,d2 = dilation\n",
        "\n",
        "  image_pd = image if padding==(0,0) else add_padding(image,padding)\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  x,y = image_pd.shape\n",
        "\n",
        "  for i in range(0,(y-b)//d2,c):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(0,(x-a)//d1,r):\n",
        "\n",
        "      print(image_pd[i:i+b:d2,j:j+a:d1])\n",
        "      temp = (image_pd[i:i+b:d2,j:j+a:d1] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "ag8GkDJzc-Vw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel, dilation=(2,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "_HIaqGa0yArI",
        "outputId": "453d1401-55ed-4b68-ef43-5c54e6629d8b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4425,  0.5298],\n",
            "        [-0.2325, -1.1987]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2979582150.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfeature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2940929073.py\u001b[0m in \u001b[0;36mConvolution\u001b[0;34m(image, kernel, stride, padding, dilation)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3bIGNsuTyIbZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}