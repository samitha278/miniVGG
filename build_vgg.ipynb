{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv/pnjaOgfQVm7tceqmGLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/miniVGG/blob/main/build_vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1BohYIwpIVZ9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Convolution function"
      ],
      "metadata": {
        "id": "2xiuBEVgYqg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel):\n",
        "\n",
        "  x,y = image.shape\n",
        "\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  for i in range(y-b):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(x-a):\n",
        "\n",
        "      temp = (image[i:i+b,j:j+a] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "IEuWS_j6Id_A"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel)\n"
      ],
      "metadata": {
        "id": "aprY8O8mK08d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rPfuwUIQBgB",
        "outputId": "d2036ffa-8801-4ef1-e9e3-8fda98cf345e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([29, 29])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution with stride"
      ],
      "metadata": {
        "id": "vHxs061xYu3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel,stride=(1,1)):\n",
        "\n",
        "  x,y = image.shape\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  r,c = stride\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  for i in range(0,y-b,c):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(0,x-a,r):\n",
        "\n",
        "      temp = (image[i:i+b,j:j+a] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "xQbUppBMRAlH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel,(2,1))      # 2: aloge side rows , 1: along side columns\n"
      ],
      "metadata": {
        "id": "8gnkQhdwdkx-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHRzV0k5doNH",
        "outputId": "75c7a1ed-3b59-4270-d5ec-3c40b275565b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([29, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Padding"
      ],
      "metadata": {
        "id": "R4eAXkpueKTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding(image, padding):\n",
        "\n",
        "\n",
        "\n",
        "  r,c = padding\n",
        "\n",
        "  if r>0 :\n",
        "    rows = torch.zeros((r,image.shape[1]))\n",
        "\n",
        "    image = torch.cat((rows , image , rows),dim=0)\n",
        "\n",
        "  if c>0:\n",
        "\n",
        "    columns = torch.zeros((c,image.shape[0]))\n",
        "\n",
        "    image = torch.cat((columns , image.T , columns),dim=0)\n",
        "\n",
        "\n",
        "  return image.T\n",
        "\n"
      ],
      "metadata": {
        "id": "kqn-dKa9L0vj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_padding(torch.randn(2,2),(2,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEZZngmkWXtv",
        "outputId": "2cd63ec5-d78e-4dec-f58a-2dd69567a112"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.1233, 0.3157, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.7346, 0.6829, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel,stride=(1,1),padding = (0,0)):\n",
        "\n",
        "\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  r,c = stride\n",
        "\n",
        "  image_pd = image if padding==(0,0) else add_padding(image,padding)\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  x,y = image_pd.shape\n",
        "\n",
        "  for i in range(0,y-b,c):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(0,x-a,r):\n",
        "\n",
        "      temp = (image_pd[i:i+b,j:j+a] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "8lou7779dt22"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel, padding=(2,2))\n"
      ],
      "metadata": {
        "id": "-isnZot9ZnKI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HJ2ScO0Z0fG",
        "outputId": "e547bcc2-d0d3-4996-934b-5bf34f56ca25"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([33, 33])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple adding pad func"
      ],
      "metadata": {
        "id": "JgaVXqF6bDfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding2(matrix,padding):\n",
        "\n",
        "  n,m = matrix.shape\n",
        "\n",
        "  r,c = padding\n",
        "\n",
        "  padded_matrix = torch.zeros((n+r*2,m+c*2))\n",
        "\n",
        "  padded_matrix[r:r+n,c:c+m] = matrix\n",
        "\n",
        "  return padded_matrix"
      ],
      "metadata": {
        "id": "jI4qh6KpZ2kJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_padding2(torch.randn(2,2),(2,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaRBYT3Vcs_7",
        "outputId": "ffe65ee1-30a9-4603-fa23-7032800f417c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  1.7920, -0.2366,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.4941, -1.0849,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Dialation"
      ],
      "metadata": {
        "id": "zsARmMMPdaZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image,kernel,stride=(1,1),padding = (0,0),dilation=(1,1)):\n",
        "\n",
        "\n",
        "  a,b = kernel.shape\n",
        "\n",
        "  r,c = stride\n",
        "\n",
        "  d1,d2 = dilation\n",
        "\n",
        "  image_pd = image if padding==(0,0) else add_padding2(image,padding)\n",
        "\n",
        "  feature_map = []\n",
        "\n",
        "  x,y = image_pd.shape\n",
        "\n",
        "\n",
        "  for i in range(0,x-a*d1+1,r):\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for j in range(0,y-b*d2+1,c):\n",
        "\n",
        "      temp = (image_pd[i:i+a*d1:d1,j:j+b*d2:d2] * kernel).sum(dim=(0,1))\n",
        "\n",
        "      row.append(temp.item())\n",
        "\n",
        "    feature_map.append(row)\n",
        "\n",
        "  return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "ag8GkDJzc-Vw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "\n",
        "feature_map = Convolution(image,kernel,(2,2),(2,2) ,dilation=(2,2))"
      ],
      "metadata": {
        "id": "_HIaqGa0yArI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map.shape"
      ],
      "metadata": {
        "id": "3bIGNsuTyIbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295392fd-573e-4a46-dce2-11ac92dbf0f2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test : comparision with pytorch conv2d"
      ],
      "metadata": {
        "id": "2PSgSdooBcum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "out_custom = Convolution(image, kernel, stride=(2,2), padding=(1,1), dilation=(2,2))\n",
        "\n",
        "\n",
        "img4d = image.unsqueeze(0).unsqueeze(0)       # (1,1,4,4)\n",
        "kernel4d = kernel.unsqueeze(0).unsqueeze(0) # (1,1,2,2)\n",
        "\n",
        "out_torch = F.conv2d(img4d, kernel4d, stride=(2,2), padding=(1,1), dilation=(2,2))\n",
        "\n",
        "print(\"Custom:\\n\", out_custom.shape)\n",
        "print(\"Torch:\\n\", out_torch.squeeze().shape)\n",
        "print(\"Match? ->\", torch.allclose(out_custom,out_torch.squeeze()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rxzc5-Ws5VR",
        "outputId": "f762b255-423a-4237-8e3e-5cad4d318875"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom:\n",
            " torch.Size([15, 15])\n",
            "Torch:\n",
            " torch.Size([15, 15])\n",
            "Match? -> True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All at once"
      ],
      "metadata": {
        "id": "iBcW_6DpIrsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding2(matrix,padding):\n",
        "  n,m = matrix.shape\n",
        "  r,c = padding\n",
        "\n",
        "  padded_matrix = torch.zeros((n+r*2,m+c*2))\n",
        "  padded_matrix[r:r+n,c:c+m] = matrix\n",
        "\n",
        "  return padded_matrix"
      ],
      "metadata": {
        "id": "HXDBUD8lI3lh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution(image, kernel, stride=(1,1), padding=(0,0), dilation=(1,1)):\n",
        "    a, b = kernel.shape\n",
        "    r, c = stride\n",
        "    d1, d2 = dilation\n",
        "\n",
        "    image_pd = image if padding == (0,0) else add_padding2(image, padding)\n",
        "    feature_map = []\n",
        "    x, y = image_pd.shape\n",
        "\n",
        "    for i in range(0, x - a*d1 + 1, r):\n",
        "        row = []\n",
        "        for j in range(0, y - b*d2 + 1, c):\n",
        "\n",
        "            temp = (image_pd[i:i+a*d1:d1, j:j+b*d2:d2] * kernel).sum(dim=(0,1))\n",
        "            row.append(temp.item())\n",
        "        feature_map.append(row)\n",
        "\n",
        "    return torch.tensor(feature_map)"
      ],
      "metadata": {
        "id": "K1ajUxKNBmxD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.manual_seed(220064)\n",
        "image = torch.randn((32,32))\n",
        "kernel = torch.ones((3,3)) * (9**-1)\n",
        "\n",
        "out_custom = Convolution(image, kernel, stride=(2,2), padding=(1,1), dilation=(2,2))\n",
        "\n",
        "\n",
        "img4d = image.unsqueeze(0).unsqueeze(0)       # (1,1,4,4)\n",
        "kernel4d = kernel.unsqueeze(0).unsqueeze(0) # (1,1,2,2)\n",
        "\n",
        "out_torch = F.conv2d(img4d, kernel4d, stride=(2,2), padding=(1,1), dilation=(2,2))\n",
        "\n",
        "print(\"Custom:\\n\", out_custom.shape)\n",
        "print(\"Torch:\\n\", out_torch.squeeze().shape)\n",
        "print(\"Match? ->\", torch.allclose(out_custom,out_torch.squeeze()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwys6oPrI6YL",
        "outputId": "f2276c1c-89e1-46db-df8a-aa2e2f06cd57"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom:\n",
            " torch.Size([15, 15])\n",
            "Torch:\n",
            " torch.Size([15, 15])\n",
            "Match? -> False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test nn.Conv2d"
      ],
      "metadata": {
        "id": "sMbL7SCfLKbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(200)\n",
        "\n",
        "c_l = nn.Conv2d(2,4,3)      # in channel = 2, out channel = 4, kernel size = 3x3\n",
        "\n",
        "\n",
        "sd = c_l.__dict__['_parameters']\n",
        "wei = sd['weight']\n",
        "bias = sd['bias']\n",
        "\n",
        "img = torch.randn(2,12,12)\n"
      ],
      "metadata": {
        "id": "7JuIATtbLBP5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for outout channel 1\n",
        "out = Convolution(img[0],wei[0,0]) + Convolution(img[1],wei[0,1]) + bias[0]"
      ],
      "metadata": {
        "id": "HVFT9JngLuVx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_real = c_l(img)[0]"
      ],
      "metadata": {
        "id": "OAQ5iR73LUR1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(out_real,out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQDfyyqGNggE",
        "outputId": "b12bcb70-07ae-4a54-d7a4-088c3fe33803"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX5ZN2NwQm6x",
        "outputId": "a1eabc7b-83ce-46ac-9f62-e8b450acc9ed"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.1716, -0.8489,  0.0087,  0.3687,  0.0612, -0.4664, -0.0916,  0.3448,\n",
              "         -0.3278,  0.4495],\n",
              "        [-0.9561,  0.1782,  0.3831, -0.2250, -0.4388, -0.5568, -0.3419,  0.2183,\n",
              "          0.0076, -0.6042],\n",
              "        [ 0.0471, -0.4671, -0.2238, -0.1945,  0.2181, -0.4026, -0.5394, -0.6788,\n",
              "         -0.5775, -0.8300],\n",
              "        [-0.1201,  0.0371, -0.3132, -0.2208, -0.2247, -0.3121,  0.1181, -0.3183,\n",
              "          0.2815, -1.5969],\n",
              "        [ 0.3042,  0.3495, -0.1749,  0.1566, -1.6928, -0.1662,  0.3522, -0.1735,\n",
              "         -0.7710, -0.8561],\n",
              "        [ 0.1167,  0.8531,  0.7492,  0.0081, -0.7360, -0.0125, -0.1202, -1.4806,\n",
              "          0.8252,  0.3005],\n",
              "        [-0.3631, -0.4048,  1.2756,  0.3087, -1.7028,  0.7630,  0.2855, -1.1258,\n",
              "          0.5164,  0.3389],\n",
              "        [-0.2170,  0.4665, -0.0310,  0.3007,  0.1645, -0.0165, -0.6288, -0.9614,\n",
              "         -0.2822,  0.3014],\n",
              "        [-1.0514, -0.2228, -0.3419,  0.8720, -0.4499,  0.1352, -0.1190, -0.4014,\n",
              "          0.8269,  0.5460],\n",
              "        [ 0.5875,  0.6329, -0.9254, -1.2093, -0.4766, -0.2230,  0.2021, -0.4626,\n",
              "         -0.4579, -0.0720]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0,0].shape, bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4fcAhV2bFcm",
        "outputId": "cc7ef5f2-c55f-4c78-fa79-39f9052776b0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 3]),\n",
              " Parameter containing:\n",
              " tensor([-0.2148,  0.1413, -0.0320,  0.0114], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Convolution(img[0],wei[0,0]) + Convolution(img[1],wei[0,1]) #+ torch.tensor([-0.2148])  # bias braodcasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ISjRHFBbj2U",
        "outputId": "084d888b-3b6c-40b1-830c-5554026c4ef9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9568, -0.6341,  0.2235,  0.5836,  0.2760, -0.2516,  0.1232,  0.5596,\n",
              "         -0.1130,  0.6643],\n",
              "        [-0.7413,  0.3930,  0.5980, -0.0102, -0.2240, -0.3420, -0.1271,  0.4332,\n",
              "          0.2224, -0.3893],\n",
              "        [ 0.2619, -0.2522, -0.0090,  0.0204,  0.4329, -0.1878, -0.3246, -0.4639,\n",
              "         -0.3626, -0.6152],\n",
              "        [ 0.0947,  0.2519, -0.0984, -0.0059, -0.0099, -0.0973,  0.3329, -0.1035,\n",
              "          0.4963, -1.3820],\n",
              "        [ 0.5190,  0.5643,  0.0400,  0.3714, -1.4779,  0.0486,  0.5670,  0.0413,\n",
              "         -0.5561, -0.6412],\n",
              "        [ 0.3316,  1.0679,  0.9641,  0.2230, -0.5212,  0.2023,  0.0947, -1.2658,\n",
              "          1.0401,  0.5153],\n",
              "        [-0.1482, -0.1899,  1.4905,  0.5235, -1.4880,  0.9779,  0.5003, -0.9110,\n",
              "          0.7312,  0.5538],\n",
              "        [-0.0021,  0.6814,  0.1839,  0.5156,  0.3793,  0.1983, -0.4140, -0.7465,\n",
              "         -0.0674,  0.5162],\n",
              "        [-0.8365, -0.0079, -0.1271,  1.0868, -0.2350,  0.3500,  0.0959, -0.1865,\n",
              "          1.0418,  0.7609],\n",
              "        [ 0.8024,  0.8478, -0.7106, -0.9944, -0.2618, -0.0082,  0.4169, -0.2478,\n",
              "         -0.2430,  0.1428]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv2d Class"
      ],
      "metadata": {
        "id": "slGCyBanLXwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Conv2d()\n",
        "\n",
        "\"\"\"\n",
        "in_channels: int, out_channels: int, kernel_size: _size_2_t,\n",
        "stride: _size_2_t = 1, padding: _size_2_t | str = 0, dilation: _size_2_t = 1,\n",
        "groups: int = 1, bias: bool = True, padding_mode: str = \"zeros\",\n",
        "device: Any | None = None, dtype: Any | None = None) -> None\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "z7nsaCzUQoLR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8ad7bb9e-842c-410e-b386-b91bb9b27a32"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nin_channels: int, out_channels: int, kernel_size: _size_2_t,\\nstride: _size_2_t = 1, padding: _size_2_t | str = 0, dilation: _size_2_t = 1,\\ngroups: int = 1, bias: bool = True, padding_mode: str = \"zeros\",\\ndevice: Any | None = None, dtype: Any | None = None) -> None\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d(nn.Module):\n",
        "\n",
        "  def __init__(self,in_channels, out_channels, kernel_size,stride = 1, padding = 0, dilation= 1,bias = True):\n",
        "    super().__init__()\n",
        "\n",
        "    assert kernel_size%2==1 , \"kernel size must be odd\"\n",
        "\n",
        "    self.weights = torch.randn((out_channels,in_channels,kernel_size,kernel_size)) * (in_channels**-0.5)\n",
        "    self.bias = torch.randn(out_channels) if bias else None\n",
        "\n",
        "\n",
        "\n",
        "    #variables\n",
        "    self.out_channels = out_channels\n",
        "    self.in_channels = in_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    self.dilation = dilation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    B,C,H,W = x.shape\n",
        "\n",
        "    H_out= W_out = ((H+2*self.padding-(self.dilation*self.kernel_size-1)-1)//self.stride)+1    # calculate output size\n",
        "\n",
        "    x = x if self.padding == 0 else self.add_padding(x,self.padding)     #adding padding\n",
        "\n",
        "    feature_map = torch.zeros((B,self.out_channels,H_out,W_out))    #output feature map\n",
        "\n",
        "    for b in range(B):\n",
        "      for out_c in range(self.out_channels):\n",
        "\n",
        "        feature_map[b,out_c,:,:] = torch.add(*[self.Convolution(x[b,i],self.weights[out_c,i]) for i in range(self.in_channels)])\n",
        "        if self.bias is not None:\n",
        "          feature_map[b,out_c,:,:] += self.bias[out_c]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return feature_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def Convolution(self,x, kernel):\n",
        "\n",
        "    a, b = kernel.shape\n",
        "    s = self.stride\n",
        "    d1 = self.dilation\n",
        "\n",
        "    feature_map = []\n",
        "    r, c = x.shape\n",
        "\n",
        "    for i in range(0, r - a*d1 + 1, s):\n",
        "        row = []\n",
        "        for j in range(0, c - b*d1 + 1, s):\n",
        "\n",
        "            temp = (x[i:i+a*d1:d1, j:j+b*d1:d1] * kernel).sum(dim=(0,1))\n",
        "            row.append(temp)\n",
        "        feature_map.append(row)\n",
        "\n",
        "    return torch.tensor(feature_map)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def add_padding(self,x,padding):\n",
        "    B,C,H,W = x.shape\n",
        "    p = padding\n",
        "\n",
        "    padded_x = torch.zeros((B,C,H+p*2,W+p*2))\n",
        "    padded_x[:,:,p:p+H,p:p+W] = x\n",
        "\n",
        "    return padded_x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iEg1eSdPJFIP"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((2,2,16,16))\n",
        "\n",
        "\n",
        "\n",
        "Conv2d_custom = Conv2d(2, 4, 3 , stride=2, padding=2, dilation=2)\n",
        "out_custom = Conv2d_custom(image)\n",
        "\n",
        "\n",
        "out_custom.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ns62G7zNYi7",
        "outputId": "b826bedf-1c35-4165-a43e-ae80de1a057a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(220064)\n",
        "image = torch.randn((2,2,16,16))\n",
        "\n",
        "\n",
        "\n",
        "#my conv2d\n",
        "start_time1 = time.time()\n",
        "\n",
        "Conv2d_custom = Conv2d(2, 4, 3 ,stride=2, padding=2, dilation=2)\n",
        "out_custom = Conv2d_custom(image)\n",
        "\n",
        "end_time1 = time.time()\n",
        "\n",
        "elapsed_time1 = end_time1 - start_time1\n",
        "\n",
        "\n",
        "\n",
        "#torch conv2d\n",
        "start_time2 = time.time()\n",
        "\n",
        "Conv2d= nn.Conv2d(2, 4, 3 ,stride=2, padding=2, dilation=2)\n",
        "out_torch = Conv2d(image)\n",
        "\n",
        "end_time2 = time.time()\n",
        "\n",
        "\n",
        "elapsed_time2 = end_time2 - start_time2\n",
        "\n",
        "\n",
        "\n",
        "print(\"Custom:\\n\", out_custom.shape , elapsed_time1)\n",
        "print(\"Torch:\\n\", out_torch.shape, elapsed_time2)\n",
        "print(\"Match? ->\", torch.allclose(out_custom,out_torch))"
      ],
      "metadata": {
        "id": "zgiMTyF2YZP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891afa2e-8c9d-4f50-d093-3fc800f47ee3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom:\n",
            " torch.Size([2, 4, 8, 8]) 0.03065347671508789\n",
            "Torch:\n",
            " torch.Size([2, 4, 8, 8]) 0.09654116630554199\n",
            "Match? -> False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTaqiyRS3-_A"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}